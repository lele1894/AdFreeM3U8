#@title è§†é¢‘è¯†åˆ«é…éŸ³-1.1
#!/usr/bin/env python3
# -*- coding: utf-8 -*-

# å®‰è£…å¿…è¦çš„ä¾èµ–
import subprocess
import sys
import re

def check_package_installed(package_name):
    """æ£€æŸ¥åŒ…æ˜¯å¦å·²å®‰è£…"""
    try:
        __import__(package_name.replace('-', '_').split('>=')[0].split('==')[0])
        return True
    except ImportError:
        return False

def check_cuda_dependencies():
    """æ£€æŸ¥å¹¶å®‰è£…CUDAç›¸å…³ä¾èµ–"""
    print("æ£€æŸ¥CUDAç›¸å…³ä¾èµ–...")
    try:
        # æ£€æŸ¥æ˜¯å¦åœ¨Linuxç¯å¢ƒä¸‹
        if os.name != 'posix':
            print("éLinuxç¯å¢ƒï¼Œè·³è¿‡CUDAä¾èµ–æ£€æŸ¥")
            return
            
        # æ£€æŸ¥æ˜¯å¦å·²å®‰è£…libcudnn8
        result = subprocess.run(['dpkg', '-l', 'libcudnn8'], stdout=subprocess.PIPE, stderr=subprocess.PIPE)
        if result.returncode != 0:
            print("æ­£åœ¨å®‰è£…CUDAä¾èµ–...")
            # æ›´æ–°åŒ…åˆ—è¡¨
            subprocess.run(['apt-get', 'update'], check=True)
            # å®‰è£…libcudnn8å’Œlibcudnn8-dev
            subprocess.run(['apt-get', 'install', '-y', 'libcudnn8', 'libcudnn8-dev'], check=True)
            print("âœ“ CUDAä¾èµ–å®‰è£…æˆåŠŸ")
        else:
            print("âœ“ CUDAä¾èµ–å·²å®‰è£…")
            
    except Exception as e:
        print(f"è­¦å‘Š: CUDAä¾èµ–å®‰è£…å¤±è´¥: {str(e)}")
        print("è¿™å¯èƒ½ä¼šå½±å“GPUåŠ é€ŸåŠŸèƒ½ï¼Œä½†ç¨‹åºä»å¯ç»§ç»­è¿è¡Œ")

def install_dependencies():
    """å®‰è£…è¿è¡Œç¯å¢ƒæ‰€éœ€çš„æ‰€æœ‰ä¾èµ–"""
    print("æ£€æŸ¥å¹¶å®‰è£…å¿…è¦çš„ä¾èµ–...")
    
    # é¦–å…ˆæ£€æŸ¥CUDAç›¸å…³ä¾èµ–
    check_cuda_dependencies()
    
    dependencies = {
        #"whisperx": "git+https://github.com/m-bain/whisperx.git",
        "whisperx": "whisperx",
        "torch": "torch",
        "torchaudio": "torchaudio",
        "ffmpeg": "ffmpeg-python",
        "deep_translator": "deep-translator",
        "edge_tts": "edge-tts",
        "pydub": "pydub",
        "nest_asyncio": "nest_asyncio",
        "langdetect": "langdetect"
    }

    for package_name, install_name in dependencies.items():
        if not check_package_installed(package_name):
            print(f"å®‰è£… {install_name}...")
            try:
                subprocess.run([sys.executable, "-m", "pip", "install", install_name], check=True)
                print(f"âœ“ {install_name} å®‰è£…æˆåŠŸ")
            except subprocess.CalledProcessError as e:
                print(f"âœ— {install_name} å®‰è£…å¤±è´¥: {str(e)}")
        else:
            print(f"âœ“ {install_name} å·²å®‰è£…")

# é¦–å…ˆå®‰è£…ä¾èµ–
install_dependencies()

# ç„¶åå¯¼å…¥æ‰€éœ€çš„æ¨¡å—
import whisperx
import gc
import os
from tqdm import tqdm
from pathlib import Path
import datetime
from deep_translator import GoogleTranslator
import time
import asyncio
import edge_tts
from pydub import AudioSegment
from concurrent.futures import ThreadPoolExecutor
import nest_asyncio
import shutil
import torch


# åº”ç”¨nest_asyncio
nest_asyncio.apply()

def check_dependencies():
    """æ£€æŸ¥å¿…è¦çš„ä¾èµ–æ˜¯å¦éƒ½å·²å®‰è£…"""
    missing_deps = []
    dependencies = {
        "whisperx": "whisperx",
        "ffmpeg": "ffmpeg-python",
        "deep_translator": "deep-translator",
        "edge_tts": "edge-tts",
        "langdetect": "langdetect"
    }

    for package_name in dependencies:
        if not check_package_installed(package_name):
            missing_deps.append(dependencies[package_name])

    if missing_deps:
        print("ä»¥ä¸‹ä¾èµ–ç¼ºå¤±ï¼Œæ­£åœ¨å®‰è£…ï¼š")
        for dep in missing_deps:
            print(f"å®‰è£… {dep}...")
            try:
                subprocess.run([sys.executable, "-m", "pip", "install", dep], check=True)
                print(f"âœ“ {dep} å®‰è£…æˆåŠŸ")
            except subprocess.CalledProcessError as e:
                print(f"âœ— {dep} å®‰è£…å¤±è´¥: {str(e)}")
                raise RuntimeError(f"ä¾èµ–å®‰è£…å¤±è´¥: {dep}")

def translate_text(text, target_lang='zh-CN', max_retries=10, retry_delay=2):
    """ç¿»è¯‘å•æ¡æ–‡æœ¬ï¼Œæ·»åŠ æ›´å¤šé‡è¯•æ¬¡æ•°"""
    for attempt in range(max_retries):
        try:
            translator = GoogleTranslator(source='auto', target=target_lang)
            translated = translator.translate(text)

            # éªŒè¯ç¿»è¯‘ç»“æœä¸ä¸ºç©ºä¸”ä¸ç­‰äºåŸæ–‡
            if translated and translated.strip() and translated != text:
                return translated
            else:
                raise Exception("ç¿»è¯‘ç»“æœæ— æ•ˆ")

        except Exception as e:
            if attempt < max_retries - 1:
                print(f"ç¿»è¯‘å‡ºé”™ (å°è¯• {attempt + 1}/{max_retries}): {str(e)}")
                print(f"ç­‰å¾… {retry_delay} ç§’åé‡è¯•...")
                time.sleep(retry_delay)
                # å¢åŠ é‡è¯•å»¶è¿Ÿæ—¶é—´ï¼Œé¿å…é¢‘ç¹è¯·æ±‚
                retry_delay = min(retry_delay * 1.5, 10)
                continue
            print(f"ç¿»è¯‘å¤±è´¥: {str(e)}")
            return text

def translate_batch(texts, target_lang='zh-CN', batch_size=5, max_retries=10, retry_delay=2):
    """æ‰¹é‡ç¿»è¯‘æ–‡æœ¬ï¼Œå‡å°æ‰¹é‡å¤§å°å¹¶å¢åŠ é‡è¯•æ¬¡æ•°"""
    results = []

    for i in range(0, len(texts), batch_size):
        batch = texts[i:i + batch_size]
        current_retry_delay = retry_delay

        for attempt in range(max_retries):
            try:
                translator = GoogleTranslator(source='auto', target=target_lang)
                translated = []

                # é€ä¸ªç¿»è¯‘æ‰¹æ¬¡ä¸­çš„æ–‡æœ¬
                for text in batch:
                    trans = translator.translate(text)
                    # éªŒè¯ç¿»è¯‘ç»“æœ
                    if not trans or not trans.strip() or trans == text:
                        raise Exception(f"æ— æ•ˆçš„ç¿»è¯‘ç»“æœ: {text} -> {trans}")
                    translated.append(trans)
                    # æ·»åŠ çŸ­æš‚å»¶è¿Ÿé¿å…è¯·æ±‚è¿‡å¿«
                    time.sleep(0.5)

                results.extend(translated)
                break

            except Exception as e:
                if attempt < max_retries - 1:
                    print(f"æ‰¹é‡ç¿»è¯‘å‡ºé”™ (å°è¯• {attempt + 1}/{max_retries}): {str(e)}")
                    print(f"ç­‰å¾… {current_retry_delay} ç§’åé‡è¯•...")
                    time.sleep(current_retry_delay)
                    # å¢åŠ é‡è¯•å»¶è¿Ÿæ—¶é—´
                    current_retry_delay = min(current_retry_delay * 1.5, 10)
                    continue

                print(f"æ‰¹é‡ç¿»è¯‘å¤±è´¥ï¼Œåˆ‡æ¢åˆ°å•æ¡ç¿»è¯‘æ¨¡å¼: {str(e)}")
                # å¦‚æœæ‰¹é‡ç¿»è¯‘å¤±è´¥ï¼Œåˆ‡æ¢åˆ°å•æ¡ç¿»è¯‘
                for text in batch:
                    translated = translate_text(text, target_lang, max_retries, retry_delay)
                    results.append(translated)
                break

    return results

def print_step(step, total_steps, message):
    """æ‰“å°å¸¦æœ‰è¿›åº¦çš„ç¾åŒ–æ¶ˆæ¯"""
    print(f"\n[{step}/{total_steps}] ğŸš€ {message}")
    print("=" * 50)
    return time.time()  # è¿”å›å¼€å§‹æ—¶é—´

def print_time_cost(start_time, task_name):
    """æ‰“å°ä»»åŠ¡è€—æ—¶"""
    cost = time.time() - start_time
    if cost < 60:
        print(f"â±ï¸ {task_name}è€—æ—¶: {cost:.1f}ç§’")
    else:
        minutes = int(cost // 60)
        seconds = cost % 60
        print(f"â±ï¸ {task_name}è€—æ—¶: {minutes}åˆ†{seconds:.1f}ç§’")

async def generate_speech(text, output_file, voice="zh-TW-HsiaoChenNeural", rate="+50%", max_retries=3, retry_delay=2):
    """ä½¿ç”¨Edge TTSç”Ÿæˆè¯­éŸ³ï¼Œæ·»åŠ é‡è¯•æœºåˆ¶"""
    for attempt in range(max_retries):
        try:
            communicate = edge_tts.Communicate(text, voice, rate=rate)
            await communicate.save(output_file)

            # éªŒè¯ç”Ÿæˆçš„éŸ³é¢‘æ–‡ä»¶
            if os.path.exists(output_file) and os.path.getsize(output_file) > 0:
                return
            else:
                raise Exception("ç”Ÿæˆçš„éŸ³é¢‘æ–‡ä»¶æ— æ•ˆ")

        except Exception as e:
            if attempt < max_retries - 1:
                print(f"è¯­éŸ³ç”Ÿæˆå‡ºé”™ (å°è¯• {attempt + 1}/{max_retries}): {str(e)}")
                print(f"ç­‰å¾… {retry_delay} ç§’åé‡è¯•...")
                await asyncio.sleep(retry_delay)
                continue
            raise Exception(f"è¯­éŸ³ç”Ÿæˆå¤±è´¥: {str(e)}")

def generate_chinese_srt(segments, output_path, target_lang='zh-CN'):
    """ç”Ÿæˆä¸­æ–‡å­—å¹•æ–‡ä»¶ï¼Œæ·»åŠ ç¿»è¯‘éªŒè¯"""
    print("æ­£åœ¨ç”Ÿæˆä¸­æ–‡å­—å¹•...")
    translated_segments = []

    # æå–æ‰€æœ‰æ–‡æœ¬è¿›è¡Œæ‰¹é‡ç¿»è¯‘
    texts = [segment["text"].strip() for segment in segments]

    # ä½¿ç”¨æ›´å°çš„æ‰¹é‡å¤§å°
    batch_size = 5
    print(f"å°†åˆ†{len(texts)//batch_size + 1}æ‰¹è¿›è¡Œç¿»è¯‘ï¼Œæ¯æ‰¹{batch_size}æ¡...")

    translated_texts = translate_batch(texts, target_lang, batch_size=batch_size)

    # éªŒè¯ç¿»è¯‘ç»“æœ
    for i, (original, translated) in enumerate(zip(texts, translated_texts)):
        if not translated or translated == original:
            print(f"è­¦å‘Š: ç¬¬{i+1}æ¡ç¿»è¯‘å¯èƒ½æœ‰é—®é¢˜ï¼Œé‡æ–°ç¿»è¯‘...")
            translated_texts[i] = translate_text(original, target_lang)

    with open(output_path, "w", encoding="utf-8") as f:
        for i, (segment, translated_text) in enumerate(zip(segments, translated_texts), start=1):
            start_time = str(datetime.timedelta(seconds=segment["start"]))[:11].replace(".", ",")
            end_time = str(datetime.timedelta(seconds=segment["end"]))[:11].replace(".", ",")

            f.write(f"{i}\n{start_time} --> {end_time}\n{translated_text}\n\n")

            # ä¿å­˜ç¿»è¯‘åçš„ç‰‡æ®µä¿¡æ¯
            translated_segments.append({
                "start": segment["start"],
                "end": segment["end"],
                "text": translated_text
            })

    return translated_segments

def extract_audio(video_path):
    """ä»è§†é¢‘æ–‡ä»¶ä¸­æå–éŸ³é¢‘"""
    audio_path = video_path.rsplit(".", 1)[0] + ".wav"
    print(f"æ­£åœ¨ä»è§†é¢‘æå–éŸ³é¢‘: {video_path}")
    subprocess.run([
        "ffmpeg", "-i", video_path,
        "-vn", "-acodec", "pcm_s16le",
        "-ar", "16000", "-ac", "1",
        audio_path, "-y"
    ], check=True)
    return audio_path

async def generate_dubbed_audio(translated_segments, output_dir, voice="zh-TW-HsiaoChenNeural", rate="+50%"):
    """ç”Ÿæˆä¸­æ–‡é…éŸ³"""
    print("æ­£åœ¨ç”Ÿæˆä¸­æ–‡é…éŸ³...")

    # åˆ›å»ºä¸´æ—¶ç›®å½•
    temp_dir = os.path.join(output_dir, "temp_audio")
    os.makedirs(temp_dir, exist_ok=True)

    # ç”Ÿæˆæ¯ä¸ªç‰‡æ®µçš„éŸ³é¢‘
    successful_segments = []
    total_duration = 0
    max_retries = 3
    retry_delay = 2

    async def process_segment(index, segment):
        try:
            # ç”ŸæˆéŸ³é¢‘æ–‡ä»¶å
            audio_file = os.path.join(temp_dir, f"segment_{index:04d}.mp3")

            # æ£€æŸ¥æ–‡æœ¬æ˜¯å¦ä¸ºç©º
            if not segment["text"].strip():
                print(f"è­¦å‘Š: ç‰‡æ®µ {index} æ–‡æœ¬ä¸ºç©ºï¼Œè·³è¿‡")
                return None

            # ç”ŸæˆéŸ³é¢‘ï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼‰
            await generate_speech(segment["text"], audio_file, voice, rate, max_retries, retry_delay)

            # éªŒè¯ç”Ÿæˆçš„éŸ³é¢‘æ–‡ä»¶
            if os.path.exists(audio_file) and os.path.getsize(audio_file) > 0:
                return {
                    "file": audio_file,
                    "start": segment["start"],
                    "end": segment["end"],
                    "duration": segment["end"] - segment["start"]
                }
            else:
                print(f"è­¦å‘Š: ç‰‡æ®µ {index} éŸ³é¢‘ç”Ÿæˆå¤±è´¥")
                return None

        except Exception as e:
            print(f"è­¦å‘Š: ç”Ÿæˆç‰‡æ®µ {index} æ—¶å‡ºé”™: {str(e)}")
            return None

    try:
        # å¹¶è¡Œç”Ÿæˆæ‰€æœ‰éŸ³é¢‘ç‰‡æ®µ
        tasks = [process_segment(i, segment) for i, segment in enumerate(translated_segments)]
        results = await asyncio.gather(*tasks)

        # è¿‡æ»¤æ‰å¤±è´¥çš„ç‰‡æ®µ
        successful_segments = [seg for seg in results if seg is not None]

        if not successful_segments:
            raise RuntimeError("æ²¡æœ‰æˆåŠŸç”Ÿæˆçš„éŸ³é¢‘ç‰‡æ®µ")

        # è®¡ç®—æ€»æ—¶é•¿
        total_duration = sum(seg["duration"] for seg in successful_segments)

        print(f"æˆåŠŸç”Ÿæˆ {len(successful_segments)}/{len(translated_segments)} ä¸ªéŸ³é¢‘ç‰‡æ®µ")
        print(f"éŸ³é¢‘ç”Ÿæˆå®Œæˆï¼Œæ€»æ—¶é•¿: {total_duration:.2f}ç§’")

        # åˆå¹¶éŸ³é¢‘ç‰‡æ®µ
        output_audio = os.path.join(output_dir, "dubbed_audio.mp3")

        # åˆ›å»ºç©ºç™½éŸ³é¢‘
        silence = AudioSegment.silent(duration=int(total_duration * 1000))

        # å°†æ¯ä¸ªç‰‡æ®µæ’å…¥åˆ°æ­£ç¡®çš„ä½ç½®
        for segment in successful_segments:
            try:
                audio_segment = AudioSegment.from_mp3(segment["file"])
                position = int(segment["start"] * 1000)
                silence = silence.overlay(audio_segment, position=position)
            except Exception as e:
                print(f"è­¦å‘Š: åˆå¹¶éŸ³é¢‘ç‰‡æ®µæ—¶å‡ºé”™: {str(e)}")
                continue

        # å¯¼å‡ºæœ€ç»ˆéŸ³é¢‘
        silence.export(output_audio, format="mp3")

        # éªŒè¯æœ€ç»ˆéŸ³é¢‘
        if not os.path.exists(output_audio) or os.path.getsize(output_audio) == 0:
            raise RuntimeError("æœ€ç»ˆéŸ³é¢‘æ–‡ä»¶ç”Ÿæˆå¤±è´¥")

        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        shutil.rmtree(temp_dir, ignore_errors=True)

        return output_audio

    except Exception as e:
        print(f"é”™è¯¯: {str(e)}")
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        shutil.rmtree(temp_dir, ignore_errors=True)
        return None

def mix_audio_and_merge_video(video_path, dubbed_audio_path, output_dir):
    """æ··åˆéŸ³é¢‘å¹¶ä¸è§†é¢‘åˆå¹¶"""
    print("æ­£åœ¨æ··åˆéŸ³é¢‘å¹¶åˆæˆæœ€ç»ˆè§†é¢‘...")

    # éªŒè¯é…éŸ³æ–‡ä»¶
    if not dubbed_audio_path or not os.path.exists(dubbed_audio_path) or os.path.getsize(dubbed_audio_path) == 0:
        raise RuntimeError("é…éŸ³æ–‡ä»¶ä¸å­˜åœ¨æˆ–ä¸ºç©º")

    try:
        # 1. è·å–è§†é¢‘æ—¶é•¿
        video_duration_cmd = f'ffprobe -v error -show_entries format=duration -of default=noprint_wrappers=1:nokey=1 "{video_path}"'
        video_duration = float(subprocess.check_output(video_duration_cmd, shell=True).decode().strip())
        print(f"è§†é¢‘æ—¶é•¿: {video_duration:.2f}ç§’")

        # 2. æå–åŸå§‹éŸ³é¢‘ï¼ˆä½¿ç”¨è§†é¢‘æ—¶é•¿ï¼‰
        original_audio_path = os.path.join(output_dir, "original_audio.wav")
        subprocess.run([
            "ffmpeg", "-i", video_path,
            "-vn", "-acodec", "pcm_s16le",
            "-ar", "44100", "-ac", "2",
            "-t", str(video_duration),  # ä½¿ç”¨è§†é¢‘æ—¶é•¿
            original_audio_path, "-y"
        ], check=True)

        # 3. å°†é…éŸ³è½¬æ¢ä¸ºWAVæ ¼å¼
        dubbed_wav_path = os.path.join(output_dir, "dubbed_audio.wav")
        subprocess.run([
            "ffmpeg", "-i", dubbed_audio_path,
            "-acodec", "pcm_s16le",
            "-ar", "44100", "-ac", "2",
            dubbed_wav_path, "-y"
        ], check=True)

        # éªŒè¯éŸ³é¢‘æ–‡ä»¶
        if not os.path.exists(original_audio_path) or not os.path.exists(dubbed_wav_path):
            raise RuntimeError("éŸ³é¢‘æå–å¤±è´¥")

        # 4. è¯»å–éŸ³é¢‘
        print("å¤„ç†éŸ³é¢‘...")
        original_audio = AudioSegment.from_wav(original_audio_path)
        dubbed_audio = AudioSegment.from_wav(dubbed_wav_path)

        print(f"åŸå§‹éŸ³é¢‘æ—¶é•¿: {len(original_audio)/1000:.2f}ç§’")
        print(f"é…éŸ³æ—¶é•¿: {len(dubbed_audio)/1000:.2f}ç§’")

        # åˆ›å»ºä¸è§†é¢‘ç­‰é•¿çš„ç©ºç™½éŸ³è½¨
        target_duration = int(video_duration * 1000)  # è½¬æ¢ä¸ºæ¯«ç§’
        base_track = AudioSegment.silent(duration=target_duration)

        # é™ä½åŸå§‹éŸ³é¢‘éŸ³é‡å¹¶ç¡®ä¿é•¿åº¦åŒ¹é…
        original_audio = original_audio - 20  # -20dB â‰ˆ 10% éŸ³é‡
        if len(original_audio) > target_duration:
            original_audio = original_audio[:target_duration]
        elif len(original_audio) < target_duration:
            original_audio = original_audio + AudioSegment.silent(duration=target_duration - len(original_audio))

        # å°†åŸå§‹éŸ³é¢‘å åŠ åˆ°åŸºç¡€éŸ³è½¨
        base_track = base_track.overlay(original_audio)

        # å°†é…éŸ³å åŠ åˆ°é€‚å½“ä½ç½®
        if len(dubbed_audio) > 0:
            base_track = base_track.overlay(dubbed_audio)

        # 5. ä¿å­˜æœ€ç»ˆéŸ³é¢‘
        mixed_audio_path = os.path.join(output_dir, "mixed_audio.wav")
        base_track.export(
            mixed_audio_path,
            format="wav",
            parameters=["-ar", "44100", "-ac", "2"]
        )

        # éªŒè¯æ··åˆéŸ³é¢‘
        if not os.path.exists(mixed_audio_path) or os.path.getsize(mixed_audio_path) == 0:
            raise RuntimeError("æ··åˆéŸ³é¢‘ç”Ÿæˆå¤±è´¥")

        print("éŸ³é¢‘æ··åˆå®Œæˆï¼Œå¼€å§‹åˆæˆè§†é¢‘...")

        # 6. åˆå¹¶è§†é¢‘å’Œæ··åˆéŸ³é¢‘
        output_video_path = os.path.join(output_dir, os.path.splitext(os.path.basename(video_path))[0] + "_dubbed.mp4")

        # ä½¿ç”¨ä¸€æ­¥å¤„ç†æ¥åˆæˆè§†é¢‘
        subprocess.run([
            "ffmpeg", "-i", video_path,
            "-i", mixed_audio_path,
            "-c:v", "copy",
            "-c:a", "aac",
            "-strict", "experimental",
            "-map", "0:v:0",
            "-map", "1:a:0",
            "-shortest",  # ä½¿ç”¨æœ€çŸ­çš„æµé•¿åº¦ï¼Œé¿å…éŸ³é¢‘æ¯”è§†é¢‘é•¿
            output_video_path, "-y"
        ], check=True)

        # éªŒè¯è¾“å‡ºè§†é¢‘
        if not os.path.exists(output_video_path) or os.path.getsize(output_video_path) == 0:
            raise RuntimeError("è§†é¢‘åˆæˆå¤±è´¥")

        print(f"è§†é¢‘åˆæˆå®Œæˆï¼Œè¾“å‡ºæ–‡ä»¶: {output_video_path}")
        return output_video_path

    finally:
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        temp_files = [
            original_audio_path,
            dubbed_wav_path,
            mixed_audio_path
        ]
        for file in temp_files:
            try:
                if os.path.exists(file):
                    os.remove(file)
            except Exception as e:
                print(f"è­¦å‘Š: æ¸…ç†ä¸´æ—¶æ–‡ä»¶ {file} æ—¶å‡ºé”™: {str(e)}")

def get_event_loop():
    """è·å–äº‹ä»¶å¾ªç¯"""
    try:
        loop = asyncio.get_event_loop()
        if loop.is_running():
            # å¦‚æœå½“å‰å¾ªç¯æ­£åœ¨è¿è¡Œï¼Œåˆ›å»ºæ–°çš„å¾ªç¯
            loop = asyncio.new_event_loop()
        return loop
    except RuntimeError:
        loop = asyncio.new_event_loop()
        return loop

def run_async(coro):
    """è¿è¡Œå¼‚æ­¥ä»£ç çš„è¾…åŠ©å‡½æ•°"""
    try:
        # è·å–å½“å‰äº‹ä»¶å¾ªç¯
        loop = asyncio.get_event_loop()
        if loop.is_running():
            # å¦‚æœå½“å‰å¾ªç¯æ­£åœ¨è¿è¡Œï¼Œåˆ›å»ºå¹¶ä½¿ç”¨æ–°çš„å¾ªç¯
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
        return loop.run_until_complete(coro)
    except RuntimeError:
        # å¦‚æœå‡ºç°è¿è¡Œæ—¶é”™è¯¯ï¼Œåˆ›å»ºæ–°çš„äº‹ä»¶å¾ªç¯
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        return loop.run_until_complete(coro)
    finally:
        # ç¡®ä¿å¾ªç¯è¢«å…³é—­
        try:
            loop.close()
        except:
            pass

def generate_original_srt(segments, output_path):
    """ç”ŸæˆåŸå§‹è¯­è¨€å­—å¹•æ–‡ä»¶"""
    print("æ­£åœ¨ç”ŸæˆåŸå§‹è¯­è¨€å­—å¹•...")
    with open(output_path, "w", encoding="utf-8") as f:
        for i, segment in enumerate(segments, start=1):
            start_time = str(datetime.timedelta(seconds=segment["start"]))[:11].replace(".", ",")
            end_time = str(datetime.timedelta(seconds=segment["end"]))[:11].replace(".", ",")
            text = segment["text"].strip()
            f.write(f"{i}\n{start_time} --> {end_time}\n{text}\n\n")

def detect_language(text):
    """æ£€æµ‹æ–‡æœ¬è¯­è¨€"""
    try:
        # ä½¿ç”¨langdetectåº“æ£€æµ‹è¯­è¨€
        from langdetect import detect
        return detect(text)
    except:
        try:
            # å¤‡ç”¨æ–¹æ¡ˆï¼šä½¿ç”¨GoogleTranslatoræ£€æµ‹è¯­è¨€
            translator = GoogleTranslator(source='auto', target='zh-CN')
            # é€šè¿‡å°è¯•ç¿»è¯‘æ¥è·å–æºè¯­è¨€
            translator.translate(text[:100])  # åªä½¿ç”¨å‰100ä¸ªå­—ç¬¦
            return translator._source
        except:
            return 'unknown'

def process_video(video_path, model_name="large-v2", device="cuda", batch_size=16, compute_type="float4", target_lang='zh-CN', voice="zh-TW-HsiaoChenNeural", rate="+50%"):
    """å¤„ç†è§†é¢‘å¹¶ç”Ÿæˆå­—å¹•å’Œé…éŸ³"""
    total_time_start = time.time()
    total_steps = 9
    current_step = 1

    step_time = print_step(current_step, total_steps, "åˆå§‹åŒ–è½¬å½•ä»»åŠ¡")

    # 1. æå–éŸ³é¢‘
    current_step += 1
    step_time = print_step(current_step, total_steps, "ä»è§†é¢‘æå–éŸ³é¢‘")
    audio_path = extract_audio(video_path)
    print_time_cost(step_time, "éŸ³é¢‘æå–")

    # 2. åŠ è½½æ¨¡å‹
    current_step += 1
    step_time = print_step(current_step, total_steps, "åŠ è½½ Whisper æ¨¡å‹")
    model_dir = os.path.join(os.getcwd(), "model")
    model = whisperx.load_model(
        model_name,
        device,
        compute_type=compute_type,
        download_root=model_dir
    )
    print_time_cost(step_time, "æ¨¡å‹åŠ è½½")

    # 3. åŠ è½½éŸ³é¢‘å¹¶è½¬å½•
    current_step += 1
    step_time = print_step(current_step, total_steps, "è½¬å½•éŸ³é¢‘")
    audio = whisperx.load_audio(audio_path)
    result = model.transcribe(audio, batch_size=batch_size)
    detected_language = result["language"]
    print(f"æ£€æµ‹åˆ°è¯­è¨€: {detected_language}")
    print_time_cost(step_time, "éŸ³é¢‘è½¬å½•")

    # 4. é‡Šæ”¾æ¨¡å‹å†…å­˜
    current_step += 1
    step_time = print_step(current_step, total_steps, "æ¸…ç†æ¨¡å‹å†…å­˜")
    del model
    gc.collect()
    torch.cuda.empty_cache()
    print_time_cost(step_time, "å†…å­˜æ¸…ç†")

    # 5. å¯¹é½è½¬å½•ç»“æœ
    current_step += 1
    step_time = print_step(current_step, total_steps, "å¯¹é½è½¬å½•ç»“æœ")
    model_a, metadata = whisperx.load_align_model(
        language_code=detected_language,
        device=device
    )
    result = whisperx.align(
        result["segments"],
        model_a,
        metadata,
        audio,
        device,
        return_char_alignments=False
    )
    print_time_cost(step_time, "ç»“æœå¯¹é½")

    # 6. ç”ŸæˆåŸå§‹è¯­è¨€å­—å¹•
    current_step += 1
    step_time = print_step(current_step, total_steps, "ç”ŸæˆåŸå§‹è¯­è¨€å­—å¹•")
    output_dir = os.path.dirname(video_path)
    base_name = os.path.splitext(os.path.basename(video_path))[0]
    original_srt = os.path.join(output_dir, f"{base_name}_original.srt")
    chinese_srt = os.path.join(output_dir, f"{base_name}_zh.srt")

    # ç”ŸæˆåŸå§‹è¯­è¨€å­—å¹•
    generate_original_srt(result["segments"], original_srt)
    print_time_cost(step_time, "åŸå§‹å­—å¹•ç”Ÿæˆ")

    # æ£€æŸ¥æ˜¯å¦éœ€è¦ç¿»è¯‘
    if detected_language == "zh":
        print("æ£€æµ‹åˆ°ä¸­æ–‡éŸ³é¢‘ï¼Œæ— éœ€ç¿»è¯‘")
        chinese_srt = original_srt
        translated_segments = result["segments"]
    else:
        # 7. ç”Ÿæˆä¸­æ–‡å­—å¹•
        current_step += 1
        step_time = print_step(current_step, total_steps, "ç”Ÿæˆä¸­æ–‡å­—å¹•")
        print(f"æ£€æµ‹åˆ°éä¸­æ–‡éŸ³é¢‘ ({detected_language})ï¼Œæ­£åœ¨ç¿»è¯‘ä¸ºä¸­æ–‡...")
        translated_segments = generate_chinese_srt(result["segments"], chinese_srt, target_lang)
        print_time_cost(step_time, "ä¸­æ–‡å­—å¹•ç”Ÿæˆ")

    # 8. ç”Ÿæˆé…éŸ³
    current_step += 1
    step_time = print_step(current_step, total_steps, "ç”Ÿæˆä¸­æ–‡é…éŸ³")
    dubbed_audio = run_async(generate_dubbed_audio(translated_segments, output_dir, voice, rate))
    print_time_cost(step_time, "é…éŸ³ç”Ÿæˆ")

    # 9. æ··åˆéŸ³é¢‘å¹¶åˆæˆè§†é¢‘
    current_step += 1
    step_time = print_step(current_step, total_steps, "åˆæˆæœ€ç»ˆè§†é¢‘")
    output_video = mix_audio_and_merge_video(video_path, dubbed_audio, output_dir)
    print_time_cost(step_time, "è§†é¢‘åˆæˆ")

    # 10. æ¸…ç†ä¸´æ—¶æ–‡ä»¶
    os.remove(audio_path)
    print("\nâœ¨ å¤„ç†å®Œæˆï¼")
    print_time_cost(total_time_start, "æ€»")
    print(f"ğŸ“ åŸå§‹å­—å¹•ï¼š{original_srt}")
    if detected_language != "zh":
        print(f"ğŸ“ ä¸­æ–‡å­—å¹•ï¼š{chinese_srt}")
    print(f"ğŸ”Š é…éŸ³æ–‡ä»¶ï¼š{dubbed_audio}")
    print(f"ğŸ¥ è¾“å‡ºè§†é¢‘ï¼š{output_video}")

    return original_srt, chinese_srt, dubbed_audio, output_video

def process_video_with_srt(video_path, srt_file, target_lang='zh-CN', voice="zh-TW-HsiaoChenNeural", rate="+50%"):
    """ä½¿ç”¨å·²æœ‰å­—å¹•æ–‡ä»¶å¤„ç†è§†é¢‘"""
    total_time_start = time.time()
    total_steps = 3
    current_step = 1

    # 1. è¯»å–å­—å¹•æ–‡ä»¶
    step_time = print_step(current_step, total_steps, "è¯»å–å­—å¹•æ–‡ä»¶")
    print(f"è¯»å–å­—å¹•æ–‡ä»¶: {srt_file}")
    segments = []
    detected_language = None

    try:
        with open(srt_file, 'r', encoding='utf-8') as f:
            lines = f.readlines()

        # ä½¿ç”¨ç¬¬ä¸€æ®µéç©ºå­—å¹•æ£€æµ‹è¯­è¨€
        text_for_detection = ""
        i = 0
        while i < len(lines):
            line = lines[i].strip()
            if line.isdigit():  # å­—å¹•åºå·
                i += 1
                if i < len(lines):  # æ—¶é—´è½´
                    i += 1
                    text_lines = []
                    while i < len(lines) and lines[i].strip():
                        text_lines.append(lines[i].strip())
                        i += 1
                    if text_lines:
                        text_for_detection = ' '.join(text_lines)
                        break
            i += 1

        if text_for_detection:
            try:
                detected_language = detect_language(text_for_detection)
                print(f"æ£€æµ‹åˆ°å­—å¹•è¯­è¨€: {detected_language}")
            except Exception as e:
                print(f"è¯­è¨€æ£€æµ‹å¤±è´¥: {str(e)}")
                detected_language = 'unknown'
        else:
            print("æ— æ³•ä»å­—å¹•æ–‡ä»¶ä¸­æå–æ–‡æœ¬è¿›è¡Œè¯­è¨€æ£€æµ‹")
            detected_language = 'unknown'

        # è§£æå­—å¹•æ–‡ä»¶
        i = 0
        while i < len(lines):
            line = lines[i].strip()
            if line.isdigit():  # å­—å¹•åºå·
                i += 1
                if i >= len(lines):
                    break

                # æ—¶é—´è½´
                time_line = lines[i].strip()
                if '-->' in time_line:
                    start_time, end_time = time_line.split(' --> ')
                    # è½¬æ¢æ—¶é—´ä¸ºç§’
                    start_seconds = sum(float(x) * 60 ** i for i, x in enumerate(reversed(start_time.replace(',', '.').split(':'))))
                    end_seconds = sum(float(x) * 60 ** i for i, x in enumerate(reversed(end_time.replace(',', '.').split(':'))))

                    i += 1
                    # è¯»å–æ–‡æœ¬å†…å®¹
                    text_lines = []
                    while i < len(lines) and lines[i].strip():
                        text_lines.append(lines[i].strip())
                        i += 1

                    if text_lines:  # åªæ·»åŠ æœ‰æ–‡æœ¬çš„å­—å¹•
                        segments.append({
                            "start": start_seconds,
                            "end": end_seconds,
                            "text": ' '.join(text_lines)
                        })
            i += 1

        if not segments:
            raise ValueError("æœªèƒ½ä»å­—å¹•æ–‡ä»¶ä¸­æå–åˆ°æœ‰æ•ˆå­—å¹•")

        print_time_cost(step_time, "å­—å¹•è¯»å–")

        # æ£€æŸ¥æ˜¯å¦éœ€è¦ç¿»è¯‘
        output_dir = os.path.dirname(video_path)
        base_name = os.path.splitext(os.path.basename(video_path))[0]
        chinese_srt = os.path.join(output_dir, f"{base_name}_zh.srt")

        # å¦‚æœè¯­è¨€æœªçŸ¥æˆ–éä¸­æ–‡ï¼Œæ‰§è¡Œç¿»è¯‘
        if detected_language not in ['zh', 'zh-CN', 'zh-TW']:
            # ç¿»è¯‘å­—å¹•
            current_step += 1
            step_time = print_step(current_step, total_steps, "ç¿»è¯‘å­—å¹•")
            print(f"æ£€æµ‹åˆ°éä¸­æ–‡å­—å¹• ({detected_language})ï¼Œæ­£åœ¨ç¿»è¯‘ä¸ºä¸­æ–‡...")
            translated_segments = generate_chinese_srt(segments, chinese_srt, target_lang)
            print_time_cost(step_time, "å­—å¹•ç¿»è¯‘")
        else:
            print("æ£€æµ‹åˆ°ä¸­æ–‡å­—å¹•ï¼Œæ— éœ€ç¿»è¯‘")
            chinese_srt = srt_file
            translated_segments = segments

        # 2. ç”Ÿæˆé…éŸ³
        current_step += 1
        step_time = print_step(current_step, total_steps, "ç”Ÿæˆä¸­æ–‡é…éŸ³")
        print("æ­£åœ¨ç”Ÿæˆä¸­æ–‡é…éŸ³...")
        dubbed_audio = run_async(generate_dubbed_audio(translated_segments, output_dir, voice, rate))

        if not dubbed_audio or not os.path.exists(dubbed_audio) or os.path.getsize(dubbed_audio) == 0:
            raise RuntimeError("é…éŸ³ç”Ÿæˆå¤±è´¥")

        print_time_cost(step_time, "é…éŸ³ç”Ÿæˆ")

        # 3. æ··åˆéŸ³é¢‘å¹¶åˆæˆè§†é¢‘
        current_step += 1
        step_time = print_step(current_step, total_steps, "åˆæˆæœ€ç»ˆè§†é¢‘")
        output_video = mix_audio_and_merge_video(video_path, dubbed_audio, output_dir)
        print_time_cost(step_time, "è§†é¢‘åˆæˆ")

        print("\nâœ¨ å¤„ç†å®Œæˆï¼")
        print_time_cost(total_time_start, "æ€»")

        print("\nğŸ“‹ å¤„ç†ç»“æœï¼š")
        print("=" * 50)
        print(f"ğŸ“ åŸå§‹å­—å¹•æ–‡ä»¶ï¼š{srt_file}")
        if detected_language not in ['zh', 'zh-CN', 'zh-TW']:
            print(f"ğŸ“ ä¸­æ–‡å­—å¹•æ–‡ä»¶ï¼š{chinese_srt}")
        print(f"ğŸµ é…éŸ³æ–‡ä»¶ï¼š{dubbed_audio}")
        print(f"ğŸ¥ é…éŸ³åçš„è§†é¢‘ï¼š{output_video}")

        return srt_file, chinese_srt, dubbed_audio, output_video

    except Exception as e:
        print(f"å¤„ç†å­—å¹•æ–‡ä»¶æ—¶å‡ºé”™: {str(e)}")
        raise

def main(video_file, srt_file=None, device="cuda", batch_size=16, compute_type="float16",
         target_lang='zh-CN', voice="zh-TW-HsiaoChenNeural", rate="+50%"):
    """ä¸»å‡½æ•°ï¼Œå¤„ç†è§†é¢‘å’Œå­—å¹•"""
    print("\nğŸ¬ å¼€å§‹å¤„ç†è§†é¢‘...")
    print("=" * 50)

    # æ£€æŸ¥ä¾èµ–
    check_dependencies()

    if srt_file and os.path.exists(srt_file):
        print(f"ğŸ“ æ£€æµ‹åˆ°å­—å¹•æ–‡ä»¶ï¼š{srt_file}")
        original_srt, chinese_srt, dubbed_audio, output_video = process_video_with_srt(
            video_file,
            srt_file,
            target_lang=target_lang,
            voice=voice,
            rate=rate
        )
    else:
        if srt_file:
            print(f"ğŸ” æœªæ£€æµ‹åˆ°å­—å¹•æ–‡ä»¶ï¼Œå°†ä»è§†é¢‘è¯†åˆ«å¼€å§‹å¤„ç†...")
        original_srt, chinese_srt, dubbed_audio, output_video = process_video(
            video_file,
            model_name="large-v2",
            device=device,
            batch_size=batch_size,
            compute_type=compute_type,
            target_lang=target_lang,
            voice=voice,
            rate=rate
        )

    return original_srt, chinese_srt, dubbed_audio, output_video

def get_user_input():
    """è·å–ç”¨æˆ·è¾“å…¥"""
    print("\nè¯·é€‰æ‹©åŠŸèƒ½ï¼š")
    print("1. å®Œæ•´è¿è¡Œï¼ˆè¯†åˆ«ã€ç¿»è¯‘ã€é…éŸ³ï¼‰")
    print("2. ä»…ç”ŸæˆåŸå§‹å­—å¹•")
    while True:
        choice = input("è¯·è¾“å…¥é€‰é¡¹ (1/2): ").strip()
        if choice in ['1', '2']:
            break
        print("æ— æ•ˆé€‰é¡¹ï¼Œè¯·é‡æ–°è¾“å…¥")

    # è·å–è§†é¢‘æ–‡ä»¶è·¯å¾„ï¼ˆå¿…é¡»ï¼‰
    while True:
        video_path = input("\nè¯·è¾“å…¥è§†é¢‘æ–‡ä»¶è·¯å¾„ï¼ˆå¿…é¡»ï¼‰: ").strip()
        if video_path and os.path.exists(video_path):
            break
        print("æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè¯·é‡æ–°è¾“å…¥")

    # å¦‚æœé€‰æ‹©å®Œæ•´è¿è¡Œï¼Œè·å–å…¶ä»–å‚æ•°
    srt_file = None
    voice = "zh-TW-HsiaoChenNeural"
    rate = "+50%"

    if choice == '1':
        # è·å–å­—å¹•æ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰
        srt_file = input("\nè¯·è¾“å…¥å­—å¹•æ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼Œç›´æ¥å›è½¦è·³è¿‡ï¼‰: ").strip()
        if not srt_file:
            srt_file = None
        elif not os.path.exists(srt_file):
            print("å­—å¹•æ–‡ä»¶ä¸å­˜åœ¨ï¼Œå°†ä»è§†é¢‘è¯†åˆ«å¼€å§‹å¤„ç†")
            srt_file = None

        # è·å–è¯­éŸ³é€‰é¡¹
        temp_voice = input("\nè¯·è¾“å…¥è¯­éŸ³é€‰é¡¹ï¼ˆç›´æ¥å›è½¦ä½¿ç”¨é»˜è®¤å€¼ï¼šzh-TW-HsiaoChenNeuralï¼‰: ").strip()
        if temp_voice:
            voice = temp_voice

        # è·å–è¯­é€Ÿ
        temp_rate = input("\nè¯·è¾“å…¥è¯­é€Ÿï¼ˆç›´æ¥å›è½¦ä½¿ç”¨é»˜è®¤å€¼ï¼š+50%ï¼‰: ").strip()
        if temp_rate:
            rate = temp_rate

    return choice, video_path, srt_file, voice, rate

def get_video_files(path):
    """è·å–æŒ‡å®šè·¯å¾„ä¸‹çš„æ‰€æœ‰è§†é¢‘æ–‡ä»¶"""
    video_extensions = ('.mp4', '.avi', '.mkv', '.mov', '.flv', '.wmv')
    if os.path.isfile(path):
        return [path] if path.lower().endswith(video_extensions) else []

    video_files = []
    for root, _, files in os.walk(path):
        for file in files:
            if file.lower().endswith(video_extensions):
                video_files.append(os.path.join(root, file))
    return video_files

if __name__ == "__main__":
    total_time_start = time.time()

    # è·å–ç”¨æˆ·è¾“å…¥
    choice, video_path, srt_file, voice, rate = get_user_input()

    # è·å–è¦å¤„ç†çš„è§†é¢‘æ–‡ä»¶åˆ—è¡¨
    video_files = get_video_files(video_path)

    if not video_files:
        print(f"âŒ é”™è¯¯ï¼šåœ¨è·¯å¾„ '{video_path}' ä¸­æœªæ‰¾åˆ°è§†é¢‘æ–‡ä»¶")
        sys.exit(1)

    # æ˜¾ç¤ºæ‰¾åˆ°çš„è§†é¢‘æ–‡ä»¶
    if len(video_files) > 1:
        print(f"\nğŸ“ æ‰¾åˆ° {len(video_files)} ä¸ªè§†é¢‘æ–‡ä»¶:")
        for i, video_file in enumerate(video_files, 1):
            print(f"{i}. {os.path.basename(video_file)}")
        print("\n")

    # å¤„ç†æ¯ä¸ªè§†é¢‘æ–‡ä»¶
    for i, video_file in enumerate(video_files, 1):
        if len(video_files) > 1:
            print(f"\nğŸ¬ å¤„ç†è§†é¢‘ {i}/{len(video_files)}: {os.path.basename(video_file)}")
            print("=" * 50)

        try:
            if choice == '1':
                # å®Œæ•´è¿è¡Œ
                main(
                    video_file=video_file,
                    srt_file=srt_file,
                    voice=voice,
                    rate=rate
                )
            else:
                # ä»…ç”ŸæˆåŸå§‹å­—å¹•
                print("\nğŸ¬ å¼€å§‹å¤„ç†è§†é¢‘...")
                print("=" * 50)

                # æ£€æŸ¥ä¾èµ–
                check_dependencies()

                # è®¾ç½®è®¾å¤‡å‚æ•°
                device = "cuda" if torch.cuda.is_available() else "cpu"
                batch_size = 16 if torch.cuda.is_available() else 4
                compute_type = "float16" if torch.cuda.is_available() else "int8"

                try:
                    # æå–éŸ³é¢‘
                    step_time = print_step(1, 4, "ä»è§†é¢‘æå–éŸ³é¢‘")
                    audio_path = extract_audio(video_file)
                    print_time_cost(step_time, "éŸ³é¢‘æå–")

                    # åŠ è½½æ¨¡å‹
                    step_time = print_step(2, 4, "åŠ è½½ Whisper æ¨¡å‹")
                    model_dir = os.path.join(os.getcwd(), "model")
                    model = whisperx.load_model(
                        "large-v2",
                        device,
                        compute_type=compute_type,
                        download_root=model_dir
                    )
                    print_time_cost(step_time, "æ¨¡å‹åŠ è½½")

                    # è½¬å½•éŸ³é¢‘
                    step_time = print_step(3, 4, "è½¬å½•éŸ³é¢‘")
                    audio = whisperx.load_audio(audio_path)
                    result = model.transcribe(audio, batch_size=batch_size)
                    detected_language = result["language"]
                    print_time_cost(step_time, "éŸ³é¢‘è½¬å½•")

                    # ç”Ÿæˆå­—å¹•æ–‡ä»¶
                    step_time = print_step(4, 4, "ç”Ÿæˆå­—å¹•æ–‡ä»¶")
                    output_dir = os.path.dirname(video_file)
                    base_name = os.path.splitext(os.path.basename(video_file))[0]
                    original_srt = os.path.join(output_dir, f"{base_name}_original.srt")

                    # ç”ŸæˆåŸå§‹è¯­è¨€å­—å¹•
                    generate_original_srt(result["segments"], original_srt)
                    print_time_cost(step_time, "å­—å¹•ç”Ÿæˆ")

                    print("\nâœ¨ å¤„ç†å®Œæˆï¼")
                    print_time_cost(total_time_start, "æ€»")
                    print(f"ğŸ“ æ£€æµ‹åˆ°è¯­è¨€: {detected_language}")
                    print(f"ğŸ“ å­—å¹•æ–‡ä»¶ï¼š{original_srt}")

                finally:
                    # æ¸…ç†èµ„æº
                    try:
                        if 'audio_path' in locals():
                            os.remove(audio_path)
                        if 'model' in locals():
                            del model
                            gc.collect()
                            torch.cuda.empty_cache()
                    except Exception as e:
                        print(f"æ¸…ç†èµ„æºæ—¶å‡ºé”™: {str(e)}")

        except Exception as e:
            print(f"\nâŒ å¤„ç†è§†é¢‘ {os.path.basename(video_file)} æ—¶å‡ºé”™:")
            print(str(e))
            if len(video_files) > 1:
                print("ç»§ç»­å¤„ç†ä¸‹ä¸€ä¸ªè§†é¢‘...\n")
                continue
            else:
                raise

    if len(video_files) > 1:
        print(f"\nâœ¨ æ‰€æœ‰è§†é¢‘å¤„ç†å®Œæˆï¼æ€»å…±å¤„ç†äº† {len(video_files)} ä¸ªè§†é¢‘æ–‡ä»¶")
        print_time_cost(total_time_start, "æ€»")