#@title M3U8ä¸‹è½½-1.1
import os
import sys
import subprocess
import requests
import re
import json
import time
import shutil
from pathlib import Path
from urllib.parse import urlparse, urljoin
from concurrent.futures import ThreadPoolExecutor, as_completed
import argparse
import warnings
import urllib3

# ç¦ç”¨SSLè­¦å‘Š
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
warnings.filterwarnings('ignore', message='Unverified HTTPS request')

def print_banner():
    """æ‰“å°ç¾åŒ–çš„æ¨ªå¹…"""
    print("\n" + "=" * 50)
    print("M3U8è§†é¢‘ä¸‹è½½åŠ©æ‰‹ v1.1")
    print("=" * 50)
    print("âœ¨ æ”¯æŒæ–­ç‚¹ç»­ä¼ ")
    print("âœ¨ è‡ªåŠ¨åˆå¹¶åˆ†ç‰‡")
    print("âœ¨ å¤šçº¿ç¨‹ä¸‹è½½åŠ é€Ÿ")
    print("âœ¨ è‡ªåŠ¨å»é™¤å¹¿å‘Š")
    print("âœ¨ æ”¯æŒä»£ç†è®¾ç½®")
    print("=" * 50 + "\n")

def print_step(message, step=None):
    """æ‰“å°å¸¦æœ‰æ­¥éª¤çš„æ¶ˆæ¯"""
    if step is None:
        print(f"[INFO] {message}")
    else:
        print(f"[{step}] {message}")

def check_ffmpeg():
    """æ£€æŸ¥ffmpegæ˜¯å¦å·²å®‰è£…"""
    try:
        result = subprocess.run(["ffmpeg", "-version"], capture_output=True, text=True)
        return result.returncode == 0
    except:
        return False

def install_dependencies():
    """å®‰è£…ä¾èµ–"""
    try:
        print_step("â¬", "æ£€æŸ¥ä¾èµ–...")
        if check_ffmpeg():
            print_step("ffmpegå·²å®‰è£…ï¼Œè·³è¿‡å®‰è£…æ­¥éª¤")
            return True
            
        print_step("â¬", "æ­£åœ¨å®‰è£…ffmpeg...")
        # åªéœ€è¦å®‰è£…ffmpeg
        subprocess.run(["apt-get", "update", "-qq"], capture_output=True)
        subprocess.run(["apt-get", "install", "-y", "ffmpeg"], capture_output=True)
        print_step("ä¾èµ–å®‰è£…æˆåŠŸ!")
        return True
    except Exception as e:
        print_step("âŒ", f"ä¾èµ–å®‰è£…å¤±è´¥: {str(e)}")
        return False

def analyze_m3u8(url, enable_ad_filter=True, ad_keywords=None):
    """åˆ†æm3u8æ–‡ä»¶ï¼Œæ‰¾å‡ºå¹¿å‘Šç‰‡æ®µå’Œåˆ†ç‰‡URL"""
    try:
        print_step("ğŸ”", "åˆ†æm3u8å†…å®¹...")
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        
        response = requests.get(url, headers=headers, timeout=10)
        content = response.text
        base_url = '/'.join(url.split('/')[:-1]) + '/'
        
        # ç»Ÿè®¡ä¿¡æ¯
        stats = {
            'normal': {'count': 0, 'duration': 0},
            'ad': {'count': 0, 'duration': 0}
        }
        
        segments = []
        duration = 0
        
        if not ad_keywords:
            ad_keywords = ['adjump', '/ad/', 'redtraffic']
            
        for line in content.split('\n'):
            line = line.strip()
            if not line:
                continue
                
            if line.startswith('#EXTINF:'):
                try:
                    duration = float(line.split(':')[1].split(',')[0])
                except:
                    duration = 0
                    
            elif line.startswith('#EXT-X-KEY'):
                if not line.startswith('http'):
                    key_path = re.search(r'URI="([^"]+)"', line)
                    if key_path:
                        full_key_url = urljoin(base_url, key_path.group(1))
                        line = line.replace(key_path.group(1), full_key_url)
                segments.append({'type': 'key', 'content': line})
                
            elif line.endswith('.ts') or line.endswith('.m4s') or line.endswith('.jpeg'):
                if not line.startswith('http'):
                    line = urljoin(base_url, line)
                    
                is_ad = False
                if enable_ad_filter:
                    is_ad = any(kw.lower() in line.lower() for kw in ad_keywords)
                    
                if is_ad:
                    stats['ad']['count'] += 1
                    stats['ad']['duration'] += duration
                else:
                    stats['normal']['count'] += 1
                    stats['normal']['duration'] += duration
                    segments.append({
                        'type': 'segment',
                        'url': line,
                        'duration': duration
                    })
            else:
                segments.append({'type': 'other', 'content': line})
        
        # æ‰“å°åˆ†æç»“æœ
        print_step("ğŸ“Š", "å†…å®¹åˆ†æç»“æœ:")
        print(f"    è§†é¢‘ç‰‡æ®µ: {stats['normal']['count']}ä¸ª ({stats['normal']['duration']:.1f}ç§’)")
        if stats['ad']['count'] > 0:
            print(f"    å¹¿å‘Šç‰‡æ®µ: {stats['ad']['count']}ä¸ª ({stats['ad']['duration']:.1f}ç§’)")
            ad_percentage = (stats['ad']['duration'] / (stats['normal']['duration'] + stats['ad']['duration'])) * 100
            print(f"    å¹¿å‘Šå æ¯”: {ad_percentage:.1f}%")
            
        return segments
        
    except Exception as e:
        print_step("âš ï¸", f"åˆ†æå¤±è´¥: {str(e)}")
        return None

def download_segment(seg_info):
    """ä¸‹è½½å•ä¸ªåˆ†ç‰‡ï¼Œæ”¯æŒå¤šæ¬¡é‡è¯•"""
    seg_path, url, index, total = seg_info
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    }
    
    # å¢åŠ é‡è¯•æ¬¡æ•°å’Œç­‰å¾…æ—¶é—´
    max_retries = 5
    retry_delays = [1, 2, 4, 8, 16]  # é€’å¢çš„ç­‰å¾…æ—¶é—´
    
    for retry in range(max_retries):
        try:
            # ä½¿ç”¨sessionæ¥ä¿æŒè¿æ¥
            with requests.Session() as session:
                response = session.get(
                    url,
                    headers=headers,
                    timeout=30,
                    stream=True,
                    verify=False  # å¿½ç•¥SSLè¯ä¹¦éªŒè¯
                )
                
                if response.status_code == 200:
                    # ç¡®ä¿ç›®å½•å­˜åœ¨
                    os.makedirs(os.path.dirname(seg_path), exist_ok=True)
                    
                    # åˆ†å—ä¸‹è½½
                    content_length = int(response.headers.get('content-length', 0))
                    downloaded_size = 0
                    
                    with open(seg_path, 'wb') as f:
                        for chunk in response.iter_content(chunk_size=8192):
                            if chunk:
                                f.write(chunk)
                                downloaded_size += len(chunk)
                    
                    # éªŒè¯ä¸‹è½½çš„æ–‡ä»¶å¤§å°
                    if content_length > 0 and downloaded_size != content_length:
                        raise Exception("æ–‡ä»¶å¤§å°ä¸åŒ¹é…")
                    
                    if os.path.getsize(seg_path) > 0:
                        return index, True, downloaded_size
                    else:
                        raise Exception("ä¸‹è½½çš„æ–‡ä»¶å¤§å°ä¸º0")
                
                elif response.status_code == 404:
                    print(f"\nâš ï¸ åˆ†ç‰‡ {index+1} ä¸å­˜åœ¨ (404)")
                    return index, False, 0
                else:
                    print(f"\nâš ï¸ åˆ†ç‰‡ {index+1} è¿”å›çŠ¶æ€ç : {response.status_code}")
            
            # å¦‚æœéœ€è¦é‡è¯•ï¼Œç­‰å¾…ä¸€æ®µæ—¶é—´
            if retry < max_retries - 1:
                time.sleep(retry_delays[retry])
                
        except requests.exceptions.Timeout:
            print(f"\nâš ï¸ åˆ†ç‰‡ {index+1} ä¸‹è½½è¶…æ—¶")
            if retry < max_retries - 1:
                time.sleep(retry_delays[retry])
                
        except requests.exceptions.ConnectionError:
            print(f"\nâš ï¸ åˆ†ç‰‡ {index+1} è¿æ¥é”™è¯¯")
            if retry < max_retries - 1:
                time.sleep(retry_delays[retry])
                
        except Exception as e:
            print(f"\nâš ï¸ åˆ†ç‰‡ {index+1} ä¸‹è½½é”™è¯¯: {str(e)}")
            if retry < max_retries - 1:
                time.sleep(retry_delays[retry])
    
    return index, False, 0

def download_m3u8(url, output_dir, filename, enable_ad_filter=True, thread_count=32, max_retries=10):
    """ä¸‹è½½M3U8è§†é¢‘"""
    temp_dir = None
    start_time = time.time()
    
    for retry_count in range(max_retries):
        try:
            # ç¡®ä¿å®‰è£…äº†ä¾èµ–
            if not install_dependencies():
                return False
            
            # åˆ†æm3u8å†…å®¹
            analyze_start = time.time()
            segments = analyze_m3u8(url, enable_ad_filter)
            analyze_time = time.time() - analyze_start
            print_step("â±ï¸", f"åˆ†æè€—æ—¶: {analyze_time:.1f}ç§’")
            
            if not segments:
                print_step("âš ï¸", "æ— æ³•åˆ†æè§†é¢‘ç‰‡æ®µ")
                continue  # é‡è¯•
            
            # åˆ›å»ºä¸´æ—¶ç›®å½•
            temp_dir = os.path.join(output_dir, "temp")
            os.makedirs(temp_dir, exist_ok=True)
            
            # å‡†å¤‡ä¸‹è½½ä»»åŠ¡
            download_tasks = []
            for i, seg in enumerate(segments):
                if seg['type'] == 'segment':  # åªä¸‹è½½è§†é¢‘ç‰‡æ®µ
                    seg_path = os.path.join(temp_dir, f"seg_{i:04d}.ts")
                    download_tasks.append((seg_path, seg['url'], i, len(segments)))
            
            if not download_tasks:
                print_step("âš ï¸", "æ²¡æœ‰æ‰¾åˆ°å¯ä¸‹è½½çš„è§†é¢‘ç‰‡æ®µ")
                continue  # é‡è¯•
            
            # ä½¿ç”¨çº¿ç¨‹æ± ä¸‹è½½
            download_start = time.time()
            print_step("ğŸš€", f"å¼€å§‹å¤šçº¿ç¨‹ä¸‹è½½ {len(download_tasks)} ä¸ªåˆ†ç‰‡...")
            successful_segments = []
            failed_segments = []
            downloaded_count = 0
            total_size = 0
            
            with ThreadPoolExecutor(max_workers=thread_count) as executor:
                futures = [executor.submit(download_segment, task) for task in download_tasks]
                for future in as_completed(futures):
                    index, success, size = future.result()
                    downloaded_count += 1
                    if success:
                        successful_segments.append(index)
                        total_size += size
                    else:
                        failed_segments.append(index)
                        print(f"\nâš ï¸ åˆ†ç‰‡ {index+1} ä¸‹è½½å¤±è´¥")
                        
                    # æ˜¾ç¤ºä¸‹è½½è¿›åº¦
                    progress = downloaded_count / len(download_tasks) * 100
                    size_mb = total_size / (1024 * 1024)
                    print(f"\rè¿›åº¦: {downloaded_count}/{len(download_tasks)} ({progress:.1f}%) - å·²ä¸‹è½½: {size_mb:.1f}MB", end="")
            
            print("\n")
            
            # å¦‚æœæœ‰å¤±è´¥çš„åˆ†ç‰‡ï¼Œé‡è¯•è¿™äº›åˆ†ç‰‡
            if failed_segments:
                print_step("âš ï¸", f"æœ‰ {len(failed_segments)} ä¸ªåˆ†ç‰‡ä¸‹è½½å¤±è´¥ï¼Œæ­£åœ¨é‡è¯•...")
                retry_tasks = []
                for index in failed_segments:
                    for task in download_tasks:
                        if task[2] == index:  # task[2] æ˜¯ index
                            retry_tasks.append(task)
                            break
                
                with ThreadPoolExecutor(max_workers=thread_count) as executor:
                    futures = [executor.submit(download_segment, task) for task in retry_tasks]
                    for future in as_completed(futures):
                        index, success, size = future.result()
                        if success:
                            successful_segments.append(index)
                            total_size += size
                            print(f"\ré‡è¯•æˆåŠŸ: åˆ†ç‰‡ {index+1}", end="")
                        else:
                            print(f"\nâŒ åˆ†ç‰‡ {index+1} é‡è¯•å¤±è´¥")
                
                print("\n")
            
            download_time = time.time() - download_start
            print_step("â±ï¸", f"ä¸‹è½½è€—æ—¶: {download_time:.1f}ç§’")
            
            if not successful_segments:
                print_step("âŒ", "æ²¡æœ‰æˆåŠŸä¸‹è½½çš„åˆ†ç‰‡ï¼Œå‡†å¤‡é‡è¯•...")
                continue  # é‡è¯•æ•´ä¸ªä¸‹è½½è¿‡ç¨‹
            
            # åˆå¹¶åˆ†ç‰‡
            merge_start = time.time()
            print_step("ğŸ“¦", "åˆå¹¶åˆ†ç‰‡...")
            # åˆ›å»ºæ–‡ä»¶åˆ—è¡¨
            list_file = os.path.join(temp_dir, "files.txt")
            with open(list_file, "w") as f:
                for i in sorted(successful_segments):
                    f.write(f"file 'seg_{i:04d}.ts'\n")
            
            # ä½¿ç”¨ffmpegåˆå¹¶
            final_path = os.path.join(output_dir, filename)
            merge_cmd = [
                "ffmpeg", "-y",
                "-f", "concat",
                "-safe", "0",
                "-i", list_file,
                "-c", "copy",
                "-bsf:a", "aac_adtstoasc",
                "-movflags", "+faststart",
                final_path
            ]
            
            merge_result = subprocess.run(merge_cmd, capture_output=True, text=True)
            merge_time = time.time() - merge_start
            
            if merge_result.returncode != 0:
                print_step("âš ï¸", f"åˆå¹¶å¤±è´¥: {merge_result.stderr}")
                continue  # é‡è¯•
            
            print_step("â±ï¸", f"åˆå¹¶è€—æ—¶: {merge_time:.1f}ç§’")
            
            # æ£€æŸ¥ç»“æœ
            if os.path.exists(final_path):
                size = os.path.getsize(final_path) / (1024*1024)  # è½¬æ¢ä¸ºMB
                total_time = time.time() - start_time
                print_step(f"ä¸‹è½½å®Œæˆ: {filename} ({size:.1f}MB)")
                print_step(f"æ€»è€—æ—¶: {total_time:.1f}ç§’")
                return True
            
            print_step("âš ï¸", "è¾“å‡ºæ–‡ä»¶ä¸å­˜åœ¨ï¼Œå‡†å¤‡é‡è¯•...")
            
        except Exception as e:
            print_step("âŒ", f"ä¸‹è½½å‡ºé”™: {str(e)}")
            if retry_count < max_retries - 1:
                print_step("âš ï¸", f"ç¬¬ {retry_count + 1} æ¬¡é‡è¯•å¤±è´¥ï¼Œå‡†å¤‡ä¸‹ä¸€æ¬¡é‡è¯•...")
                time.sleep(2)  # ç­‰å¾…2ç§’åé‡è¯•
            else:
                print_step("âŒ", "å·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œä¸‹è½½å¤±è´¥")
        finally:
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            if temp_dir and os.path.exists(temp_dir):
                try:
                    shutil.rmtree(temp_dir)
                except:
                    pass
    
    return False

def extract_m3u8_urls(file_path):
    """ä»æ–‡æœ¬æ–‡ä»¶ä¸­æå–m3u8é“¾æ¥"""
    urls = []
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
            # åŒ¹é…ä»¥.m3u8ç»“å°¾çš„URL
            pattern = r'https?://[^\s<>"\']+?\.m3u8'
            matches = re.findall(pattern, content)
            urls = list(set(matches))  # å»é‡
        print_step("ğŸ”", f"æ‰¾åˆ° {len(urls)} ä¸ªm3u8é“¾æ¥")
        return urls
    except Exception as e:
        print_step("âŒ", f"æå–é“¾æ¥å¤±è´¥: {str(e)}")
        return []

def analyze_m3u8_for_keywords(url):
    """åˆ†æm3u8å†…å®¹ä»¥å¯»æ‰¾å¯èƒ½çš„å¹¿å‘Šå…³é”®è¯"""
    try:
        print("\næ­£åœ¨åˆ†æM3U8å†…å®¹...")
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        
        response = requests.get(url, headers=headers, timeout=10)
        content = response.text
        lines = content.split('\n')
        
        # åˆ†æä¸è¿ç»­çš„è¡Œ
        print("\n" + "=" * 50)
        print("M3U8æ–‡ä»¶å†…å®¹åˆ†æ:")
        print("=" * 50)
        
        # æŸ¥æ‰¾ä¸è¿ç»­çš„ç‰‡æ®µ
        discontinuity_positions = []
        for i, line in enumerate(lines):
            if '#EXT-X-DISCONTINUITY' in line:
                discontinuity_positions.append(i)
        
        # æ˜¾ç¤ºä¸è¿ç»­ç‚¹çš„ä¸Šä¸‹æ–‡
        if discontinuity_positions:
            print("\nå‘ç°ä¸è¿ç»­ç‚¹:")
            for pos in discontinuity_positions:
                print("\n" + "-" * 30)
                print(f"ä¸è¿ç»­ç‚¹ä½ç½®: ç¬¬{pos + 1}è¡Œ")
                # æ˜¾ç¤ºå‰5è¡Œå’Œå5è¡Œ
                start = max(0, pos - 5)
                end = min(len(lines), pos + 6)
                for i in range(start, end):
                    prefix = ">>> " if i == pos else "    "
                    print(f"{prefix}[{i + 1:4d}] {lines[i]}")
        else:
            print("\næœªå‘ç°ä¸è¿ç»­ç‚¹")
            
        print("\n" + "=" * 50)
        
        # åˆ†æå¯èƒ½çš„å¹¿å‘Šå…³é”®è¯
        potential_keywords = set()
        for line in lines:
            if line.strip() and (line.endswith('.ts') or line.endswith('.m4s')):
                parts = line.lower().split('/')
                for part in parts:
                    if 'ad' in part or 'jump' in part or 'promo' in part:
                        potential_keywords.add(part)
        
        return potential_keywords
        
    except Exception as e:
        print(f"\nåˆ†æå¤±è´¥: {str(e)}")
        return set()

def get_user_input():
    """è·å–ç”¨æˆ·è¾“å…¥çš„æ‰€æœ‰é…ç½®"""
    print("\n" + "=" * 50)
    print("è¯·é…ç½®ä¸‹è½½é€‰é¡¹:")
    print("=" * 50)
    
    # è·å–è¾“å…¥é“¾æ¥æˆ–æ–‡ä»¶
    while True:
        input_path = input("1. è¯·è¾“å…¥m3u8é“¾æ¥æˆ–åŒ…å«é“¾æ¥çš„æ–‡æœ¬æ–‡ä»¶è·¯å¾„: ").strip()
        if input_path:
            break
        print("âŒ è¾“å…¥ä¸èƒ½ä¸ºç©ºï¼Œè¯·é‡è¯•")
    
    # å¦‚æœæ˜¯å•ä¸ªm3u8é“¾æ¥ï¼Œåˆ†æå†…å®¹
    potential_keywords = set()
    if input_path.lower().endswith('.m3u8'):
        # è¯¢é—®æ˜¯å¦éœ€è¦æ‹¼æ¥ç½‘å€
        should_concat = input("\né“¾æ¥æ˜¯å¦ä¸å®Œæ•´? (y/N): ").strip().lower() == 'y'
        if should_concat:
            # ä»URLä¸­æå–åŸºç¡€URL
            m3u8_url = input_path
            base_url = '/'.join(m3u8_url.split('/')[:-1]) + '/'
            if not input_path.startswith('http'):
                input_path = urljoin(base_url, input_path)
                print(f"å®Œæ•´é“¾æ¥: {input_path}")
        potential_keywords = analyze_m3u8_for_keywords(input_path)
    
    # è¯¢é—®å¹¿å‘Šå…³é”®è¯
    print("\n2. è¯·è¾“å…¥å¹¿å‘Šå…³é”®è¯:")
    if potential_keywords:
        print("å‘ç°å¯èƒ½çš„å¹¿å‘Šå…³é”®è¯:")
        for kw in potential_keywords:
            print(f"â€¢ {kw}")
    keywords = input("è¯·è¾“å…¥å¹¿å‘Šå…³é”®è¯(å¤šä¸ªå…³é”®è¯ç”¨é€—å·åˆ†éš”): ").strip()
    ad_keywords = [kw.strip() for kw in keywords.split(',')] if keywords else ['adjump', '/ad/', 'redtraffic']
    
    # è¯¢é—®è¾“å‡ºç›®å½•
    output_dir = input("\n3. è¯·è¾“å…¥ä¿å­˜ç›®å½• (ç›´æ¥å›è½¦ä½¿ç”¨é»˜è®¤ç›®å½•'output'): ").strip()
    if not output_dir:
        output_dir = 'output'
    
    return {
        'input_path': input_path,
        'enable_ad_filter': True,
        'ad_keywords': ad_keywords,
        'output_dir': output_dir,
        'thread_count': 32
    }

def main():
    """ä¸»å‡½æ•°"""
    print_banner()
    
    try:
        # è·å–ç”¨æˆ·é…ç½®
        config = get_user_input()
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        output_dir = os.path.join(os.getcwd(), config['output_dir'])
        os.makedirs(output_dir, exist_ok=True)
        
        # è·å–ä¸‹è½½é“¾æ¥
        urls_to_download = []
        if config['input_path'].lower().endswith('.m3u8'):
            urls_to_download = [config['input_path']]
        else:
            if not os.path.exists(config['input_path']):
                print_step("âŒ", "é”™è¯¯ï¼šæ–‡ä»¶ä¸å­˜åœ¨")
                return
            urls_to_download = extract_m3u8_urls(config['input_path'])
        
        if not urls_to_download:
            print_step("âŒ", "æœªæ‰¾åˆ°ä»»ä½•m3u8é“¾æ¥")
            return
        
        print_step("ğŸ“‚", f"è§†é¢‘å°†ä¿å­˜åˆ°: {output_dir}")
        print("=" * 50)
        
        # å¼€å§‹ä¸‹è½½
        for i, url in enumerate(urls_to_download, 1):
            print(f"\næ­£åœ¨å¤„ç†ç¬¬ {i}/{len(urls_to_download)} ä¸ªæ–‡ä»¶")
            filename = os.path.splitext(os.path.basename(urlparse(url).path))[0] + ".mp4"
            print_step("ğŸ“", f"æ–‡ä»¶å: {filename}")
            download_m3u8(
                url=url,
                output_dir=output_dir,
                filename=filename,
                enable_ad_filter=config['enable_ad_filter'],
                thread_count=config['thread_count']
            )
            print("=" * 50)
            
    except KeyboardInterrupt:
        print_step("ğŸ›‘", "ä¸‹è½½å·²å–æ¶ˆ")
    except Exception as e:
        print_step("âŒ", f"å‘ç”Ÿé”™è¯¯: {str(e)}")

if __name__ == "__main__":
    main()
